# benchmark_runner.py
import time
from tqdm import tqdm
import logging

from collections import defaultdict
from typing import List

from model_adapter import BaseModelAdapter
from evaluate import OpenAIJudger
from typing import List
from evaluate import OpenAIJudger

# logging.basicConfig(level=logging.DEBUG)

class BenchmarkRunner:
    def __init__(self, model_adapter: BaseModelAdapter, tasks: List, judger: OpenAIJudger, task_index: int = 0, benchmark_logger: logging.Logger = None):
        self.model_adapter = model_adapter
        self.tasks = tasks
        self.results = []
        self.judger = judger
        self.task_index = task_index
        self.benchmark_logger = benchmark_logger

    def run(self):
        print(f"\n\nğŸš€ Starting benchmark for model: {self.model_adapter.model_id}")
        
        total_start_time = time.time()
        self.total_execution_time = 0.0

        if self.task_index != 0:
            # å¦‚æœæŒ‡å®šäº†ç‰¹å®šä»»åŠ¡ï¼Œåˆ™åªè¿è¡Œè¯¥ä»»åŠ¡
            task = self.tasks[self.task_index - 1]
            print(f"===== Running Task: {task.get_name()} =====")
            print(f"Category: {task.get_category()}")
            print(f"Description: {task.get_description()}")
            prompt = task.generate_prompt()
            start_time = time.time()
            response = self.model_adapter.query(prompt)
            end_time = time.time()
            
            execution_time = round(end_time - start_time, 2)
            
            print(f"Model Response (took {execution_time}s): \n---\n{response}\n---\n")

            score, reason = task.evaluate(response, self.judger)
            print(f"ğŸ“Š Score: {score}/1.0")
            print(f"Reason: {reason}\n")
            
            self.results.append({
                "task_name": task.get_name(),
                "category": task.get_category(),  # âœ… æ–°å¢åˆ†ç±»
                "execution_time": execution_time,
                "score": score,
                "reason": reason,
            })
        else:
            for i, task in tqdm(enumerate(self.tasks), total=len(self.tasks), desc="Running tasks"):
                self.benchmark_logger.info(f"## Task {i+1}: {task.get_name()} ")
                self.benchmark_logger.info(f"**åˆ†ç±»**: {task.get_category()}\n")  # âœ… è®°å½•åˆ†ç±»
                self.benchmark_logger.info("### æç¤ºè¯\n")
                prompt = task.generate_prompt()
                self.benchmark_logger.info("```markdown\n" + prompt + "\n```")
                
                start_time = time.time()
                response = self.model_adapter.query(prompt)
                end_time = time.time()
                self.benchmark_logger.info("### æ¨¡å‹å“åº”\n")
                
                execution_time = 0.0  # åˆå§‹åŒ–ï¼Œé˜²æ­¢è¶…æ—¶æ—¶æœªå®šä¹‰
                
                if "Error calling" in response and "timeout" in response:
                    self.benchmark_logger.info(f"æ¨¡å‹è¶…æ—¶ï¼\n{response}\n\n")
                    score = 0
                    reason = "æ— æ³•åœ¨è§„å®šæ—¶é—´å†…ç”Ÿæˆå®Œæ•´å“åº”"
                else:
                    execution_time = round(end_time - start_time, 2)
                    self.benchmark_logger.info(f"æ¨¡å‹è¾“å‡ºè€—æ—¶ï¼š{execution_time}s\n\n")
                    self.benchmark_logger.info(f"æ¨¡å‹è¾“å‡ºï¼š\n")
                    self.benchmark_logger.info("```markdown\n" + response + "\n```\n")
                    
                    score, reason = task.evaluate(response, self.judger)
                
                self.benchmark_logger.info("### è¯„ä»·ç»“æœ\n")
                self.benchmark_logger.info(f"ğŸ“Šå›ç­”è¯„åˆ†: **{score}**\n")
                self.benchmark_logger.info(f"è¯„åˆ†ç†ç”±: {reason}\n")
                
                self.results.append({
                    "task_name": task.get_name(),
                    "category": task.get_category(),
                    "execution_time": execution_time,
                    "score": score,
                    "reason": reason,
                })
                self.total_execution_time += execution_time
            
        total_end_time = time.time()
        self.total_benchmark_time = round(total_end_time - total_start_time, 2)
        print(f"âœ… Benchmark finished in {self.total_benchmark_time}s.")
        return self.get_summary()

    def get_summary(self):
        """
        æŒ‰ç±»åˆ«æ±‡æ€»ç»Ÿè®¡ï¼Œè¾“å‡ºæ¯ä¸ªç±»åˆ«çš„å¹³å‡åˆ†ã€‚
        """
        # ============ 1. æŒ‰ç±»åˆ«åˆ†ç»„ç»Ÿè®¡ ============
        category_scores = defaultdict(list)
        for res in self.results:
            category_scores[res["category"]].append(res["score"])
        
        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„å¹³å‡åˆ†
        category_avg = {}
        for category, scores in category_scores.items():
            avg = round(sum(scores) / len(scores), 2) if scores else 0
            category_avg[category] = {
                "average": avg,
                "count": len(scores),
                "total": round(sum(scores), 2)
            }
        
        # æŒ‰ç±»åˆ«åæ’åºï¼Œä¿è¯è¾“å‡ºé¡ºåºä¸€è‡´
        sorted_categories = sorted(category_avg.keys())
        
        # ============ 2. è®¡ç®—æ€»å¹³å‡åˆ† ============
        total_score = sum(res["score"] for res in self.results)
        total_count = len(self.results)
        overall_average = round(total_score / total_count, 2) if total_count > 0 else 0
        
        # ============ 3. ç”Ÿæˆ Markdown è¡¨æ ¼ï¼ˆæŒ‰ç±»åˆ«ï¼‰ ============
        # è¡¨å¤´ï¼š| æ¨¡å‹å | ç±»åˆ«1 | ç±»åˆ«2 | ... | æ€»å¹³å‡åˆ† | è€—æ—¶(s) |
        header_row = "| æ¨¡å‹å | " + " | ".join(sorted_categories) + " | æ€»å¹³å‡åˆ† | è€—æ—¶(s) |"
        
        # åˆ†å‰²çº¿
        separator_row = "|---" * (len(sorted_categories) + 3) + "|"
        
        # æ•°æ®è¡Œï¼šå„ç±»åˆ«å¹³å‡åˆ†
        category_scores_str = [str(category_avg[cat]["average"]) for cat in sorted_categories]
        data_row = f"| {self.model_adapter.model_id} | " + " | ".join(category_scores_str) + f" | {overall_average} | {self.total_execution_time} |"
        
        # ============ 4. æ‰“å°è¯¦ç»†æ—¥å¿— ============
        self.benchmark_logger.info("## æœ€ç»ˆè¯„ä»·æ‘˜è¦\n")
        self.benchmark_logger.info(f"æµ‹è¯„æ¨¡å‹: {self.model_adapter.model_id}\n")
        self.benchmark_logger.info(f"æµ‹è¯„è€—æ—¶: {self.total_benchmark_time}s\n")
        self.benchmark_logger.info(f"ğŸ“Š æ€»å¹³å‡åˆ†: {overall_average}\n\n")
        
        # æ‰“å°å„ç±»åˆ«è¯¦æƒ…
        self.benchmark_logger.info("### å„ç±»åˆ«å¾—åˆ†è¯¦æƒ…\n")
        self.benchmark_logger.info("| ç±»åˆ« | ä»»åŠ¡æ•° | ç±»åˆ«æ€»åˆ† | ç±»åˆ«å¹³å‡åˆ† |")
        self.benchmark_logger.info("|---|---|---|---|")
        for cat in sorted_categories:
            info = category_avg[cat]
            self.benchmark_logger.info(f"| {cat} | {info['count']} | {info['total']} | {info['average']} |")
        self.benchmark_logger.info("\n")
        
        # æ‰“å°æ±‡æ€»è¡¨æ ¼
        self.benchmark_logger.info("### æ±‡æ€»è¡¨æ ¼\n")
        self.benchmark_logger.info(f"{header_row}\n{separator_row}\n{data_row}\n")

        # ============ 5. è¿”å›ç»“æ„åŒ– Summary ============
        summary = {
            "model_id": self.model_adapter.model_id,
            "total_tasks": total_count,
            "overall_average": overall_average,
            "total_execution_time": self.total_execution_time,
            "total_benchmark_time": self.total_benchmark_time,
            "category_summary": category_avg,  # âœ… å„ç±»åˆ«ç»Ÿè®¡
        }
        return summary