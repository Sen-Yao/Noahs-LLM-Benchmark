# benchmark_runner.py
import time
from tqdm import tqdm
import logging

from model_adapter import BaseModelAdapter
from evaluate import OpenAIJudger
from typing import List
from evaluate import OpenAIJudger

# logging.basicConfig(level=logging.DEBUG)

class BenchmarkRunner:
    def __init__(self, model_adapter: BaseModelAdapter, tasks: List, judger: OpenAIJudger, task_index: int = 0, benchmark_logger: logging.Logger = None):
        self.model_adapter = model_adapter
        self.tasks = tasks
        self.results = []
        self.judger = judger
        self.task_index = task_index
        self.benchmark_logger = benchmark_logger

    def run(self):
        print(f"\n\nğŸš€ Starting benchmark for model: {self.model_adapter.model_id}")
        
        
        total_start_time = time.time()
        self.total_execution_time = 0.0

        if self.task_index != 0:
            # å¦‚æœæŒ‡å®šäº†ç‰¹å®šä»»åŠ¡ï¼Œåˆ™åªè¿è¡Œè¯¥ä»»åŠ¡
            task = self.tasks[self.task_index - 1]
            print(f"===== Running Task: {task.get_name()} =====")
            print(f"Description: {task.get_description()}")
            prompt = task.generate_prompt()
            start_time = time.time()
            response = self.model_adapter.query(prompt)
            end_time = time.time()
            
            execution_time = round(end_time - start_time, 2)
            
            print(f"Model Response (took {execution_time}s): \n---\n{response}\n---\n")

            score, reason = task.evaluate(response, self.judger)
            print(f"ğŸ“Š Score: {score}/1.0")
            print(f"Reason: {reason}\n")
            
            self.results.append({
                "task_name": task.get_name(),
                "execution_time": execution_time,
                # "prompt": prompt,
                # "response": response,
                "score": score,
                "reason": reason,
            })
        else:
            for i, task in tqdm(enumerate(self.tasks), total=len(self.tasks), desc="Running tasks"):
                # print(f"===== Running Task {i+1}/{len(self.tasks)}: {task.get_name()} =====")
                # print(f"Description: {task.get_description()}")
                self.benchmark_logger.info(f"## Task {i+1}: {task.get_name()} ")
                self.benchmark_logger.info("### æç¤ºè¯\n")
                prompt = task.generate_prompt()
                self.benchmark_logger.info("```markdown\n" + prompt + "\n```")
                
                start_time = time.time()
                response = self.model_adapter.query(prompt)
                end_time = time.time()
                self.benchmark_logger.info("### æ¨¡å‹å“åº”\n")
                
                if "Error calling" in response and "timeout" in response:
                    self.benchmark_logger.info(f"æ¨¡å‹è¶…æ—¶ï¼\n{response}\n\n")

                    # ç›´æ¥è¯„ä¸º 0 åˆ†ï¼Œå› ä¸ºæ— æ³•åœ¨è§„å®šæ—¶é—´å†…ç”Ÿæˆå®Œæ•´å“åº”
                    score = 0
                    reason = "æ— æ³•åœ¨è§„å®šæ—¶é—´å†…ç”Ÿæˆå®Œæ•´å“åº”"

                else:
                    execution_time = round(end_time - start_time, 2)
                    self.benchmark_logger.info(f"æ¨¡å‹è¾“å‡ºè€—æ—¶ï¼š{execution_time}s\n\n")
                    self.benchmark_logger.info(f"æ¨¡å‹è¾“å‡ºï¼š\n")
                    self.benchmark_logger.info("```markdown\n" + response + "\n```\n")
                    
                    # print(f"Model Response (took {execution_time}s): \n---\n{response}\n---\n")
                    score, reason = task.evaluate(response, self.judger)
                self.benchmark_logger.info("### è¯„ä»·ç»“æœ\n")
                self.benchmark_logger.info(f"ğŸ“Šå›ç­”è¯„åˆ†: **{score}**\n")
                self.benchmark_logger.info(f"è¯„åˆ†ç†ç”±: {reason}\n")
                
                self.results.append({
                    "task_name": task.get_name(),
                    "execution_time": execution_time,
                    # "prompt": prompt,
                    # "response": response,
                    "score": score,
                    "reason": reason,
                })
                self.total_execution_time += execution_time
            
        total_end_time = time.time()
        self.total_benchmark_time = round(total_end_time - total_start_time, 2)
        print(f"âœ… Benchmark finished in {self.total_benchmark_time}s.")
        return self.get_summary()

    def get_summary(self):
        total_score = sum(res["score"] for res in self.results)
        count = len(self.tasks)
        average_score = round(total_score / count, 2) if count > 0 else 0
        
        # 1. åŠ¨æ€ç”Ÿæˆè¡¨å¤´ (Headers) 
        # å–å‡ºæ‰€æœ‰ä»»åŠ¡çš„åç§°ä½œä¸ºåˆ—å
        task_names = [task.get_name() for task in self.tasks]
        header_row = "| æ¨¡å‹å | " + " | ".join(task_names) + " | å¹³å‡åˆ† | è€—æ—¶(s) |"
        
        # 2. åŠ¨æ€ç”Ÿæˆåˆ†å‰²çº¿ (Separator)
        # æ ¹æ®åˆ—æ•°ç”Ÿæˆ |-|-|-|
        separator_row = "|---" * (len(task_names) + 3) + "|" # +3 æ˜¯å› ä¸ºæœ‰ æ¨¡å‹åã€å¹³å‡åˆ†ã€è€—æ—¶
        
        # 3. åŠ¨æ€ç”Ÿæˆåˆ†æ•°è¡Œ (Score Row)
        # æŒ‰ç…§ä»»åŠ¡é¡ºåºæ’åˆ—åˆ†æ•°ï¼ˆé‡ç‚¹ï¼šé€šè¿‡ task_id æˆ– index åŒ¹é…ç¡®ä¿å¯¹åº”ï¼‰
        # å‡è®¾ self.results æ˜¯æŒ‰ self.tasks é¡ºåºç”Ÿæˆçš„
        scores = [str(res["score"]) for res in self.results]
        data_row = f"| {self.model_adapter.model_id} | " + " | ".join(scores) + f" | {average_score} | {self.total_execution_time} |"
        
        # 4. æ‰“å°æ—¥å¿—
        self.benchmark_logger.info("## æœ€ç»ˆè¯„ä»·æ‘˜è¦\n")
        self.benchmark_logger.info(f"æµ‹è¯„æ¨¡å‹: {self.model_adapter.model_id}\n")
        self.benchmark_logger.info(f"æµ‹è¯„è€—æ—¶: {self.total_benchmark_time}s\n")
        self.benchmark_logger.info(f"ğŸ“Š å¹³å‡åˆ†: {average_score}\n")
        
        # ç»„è£…å®Œæ•´çš„ Markdown è¡¨æ ¼
        self.benchmark_logger.info(f"{header_row}\n{separator_row}\n{data_row}\n")

        summary = {
            "model_id": self.model_adapter.model_id,
            "total_tasks": count,
            "average_score": average_score,
            "results": self.results
        }
        return summary